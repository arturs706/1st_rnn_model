{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "training_samples_ = [\n",
    "\"nice shish kebab,\", \n",
    "\"amazing restaurant,\", \n",
    "\"would reccommend this place\", \n",
    "\"felt in a love!\", \n",
    "\"will attend this again\", \n",
    "\"food was crap\",\n",
    "\"please do not visit\",\n",
    "\"fuu\",\n",
    "\"crap food\",\n",
    "\"this needs new boss\",\n",
    "\"I have got bad\",\n",
    "\"Shawerma was perfect\",\n",
    "]\n",
    "# Sentiment Analysis is a predictive modelling task where the model is trained \n",
    "# to predict the polarity of textual data or sentiments like Positive, Neural, and negative. \n",
    "# Also known as labels\n",
    "\n",
    "sentiment = np.array([1,1,1,1,1,0,0,0,0,0,0,1])\n",
    "# one hot encoding does encode the data into numerical values amazing became == 2, restaurant == 18\n",
    "# 40 is the size of the vocabulary\n",
    "test_one_hot = one_hot(\"horrible food\", 50)\n",
    "print(f\"This is the test result: {test_one_hot}\")\n",
    "#assigning a vocabulary size\n",
    "vocab_size = 50\n",
    "# encoding the entire array, creating an encoded vector for each review\n",
    "encoded_training_samples_ = [one_hot(d, vocab_size) for d in training_samples_]\n",
    "# printing out the values\n",
    "encoded_training_samples_\n",
    "# Maximum length of the vector\n",
    "max_length = 4\n",
    "# adding a padding to shorter vectors, aka adding an extra 0\n",
    "padded_training_samples_ = pad_sequences(encoded_training_samples_, maxlen=max_length, padding='post')\n",
    "print(padded_training_samples_)\n",
    "\n",
    "# creating a model\n",
    "rnn_model = Sequential()\n",
    "\n",
    "# Adding the first layer to the model\n",
    "# We need to add arguments - vocab_size, vector size, the length and the label\n",
    "rnn_model.add(Embedding(vocab_size, max_length, input_length=max_length, name=\"embedding_process\"))\n",
    "# Adding the second layer with flattening\n",
    "rnn_model.add(Flatten())\n",
    "# Applying a sigmoid activation function\n",
    "rnn_model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# Training set\n",
    "X = padded_training_samples_\n",
    "# Labels\n",
    "y = sentiment\n",
    "\n",
    "# Now we need to compile our model\n",
    "# Adam is most used optimizer\n",
    "# Loss Function: Binary Cross-Entropy (Output is either Yes or No == 1 or 0)\n",
    "# Metrics == accuracy\n",
    "\n",
    "rnn_model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "# printing model summary ==>\n",
    "\n",
    "# rnn_model.summary()\n",
    "# Training a model\n",
    "rnn_model.fit(X, y, epochs=100, verbose=0)\n",
    "\n",
    "# Evaluating the model\n",
    "loss, accuracy = rnn_model.evaluate(X,y)\n",
    "\n",
    "print(f\"Model accuracy ==> {accuracy}\")\n",
    "# Getting weights\n",
    "# rnn_model.get_layer(\"embedding_process\").get_weights()[0]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
